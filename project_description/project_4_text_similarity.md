# Project 4: Text Similarity as An Evaluation Measure of Text Generation

## General Context:

Assessing the quality of natural language generation systems through human annotation is very expensive. Additionally,
human annotation campaigns are time-consuming and include non-reusable human labour. In practice, researchers rely on
automatic metrics as a proxy of quality. In the last decade, many string-based metrics (e.g., BLEU)
have been introduced. However, such metrics usually rely on exact matches and thus, do not robustly handle synonyms.
This metric can be very usefull to discriminate between generated and human text..... For example if you want to find
out that a text has been generated by ChatGPT.

## The Problem:

A plethora of applications of natural language processing (NLP) performs text-to-text transformation. Given an input,
these systems are required to produce an output text that is coherent, readable and informative. Due to both high
annotation costs and time, researchers tend to rely on automatic evaluation to compare the outputs of such systems.
Reference-based automatic evaluation relies on comparing a candidate text produced by the NLG system and one or multiple
reference texts (‘gold standard’) created by a human annotator. Generic automatic evaluation of NLG is a huge challenge
as it requires building a metric that evaluates the similarity between a candidate and one or several gold-standard
reference texts. However, the definition of success criteria is task-specific: as an example, evaluation of text
summarization focuses on content, coherence, grammatically, conciseness, and readability, whereas machine translation
focuses on fidelity, fluency and adequacy of the translation and data2text generation consider criteria such as data
coverage, correctness and text structure.

## What Exists:

Automatic text evaluation metrics fall into two categories: metrics that are trained to maximise their correlations
using human annotation and untrained metrics (e.g. BLEU, ROUGE, BERTSCORE, DepthScore [0], BaryScore [1], InfoLM [3],
MoverScore). Here you will focus on untrained metrics as trained metrics may not generalize well to new data (
existing labelled corpora are of small size). Two categories of untrained metrics can be distinguished: word or
character based-metrics that compute a score based on string representation and embedding-based metrics that rely on a
continuous representation. String-based metrics often fail to robustly match paraphrases as they mainly focus on the
surface form as opposed to embedding-based metrics relying on continuous representations.

## Problem Statement:

Given a dataset $D = \{\pmb{x_i}, \{\pmb{y_i^s},h(\pmb{x_i},\pmb{y_i^s})\}_{s=1}^S \}_{i=1}^N$ where $\pmb{x}_
i$ is the $i$-th reference text; $\pmb{y}_i^s$ is the $i$-th candidate text generated by the $s$-th NLG system; $N$ is
the number of texts in the dataset and $S$ the number of systems available. The vector $\pmb{x_i} = ({x_1,\cdots,x_M})$
is composed of M tokens (e.g., words or subwords) and $\pmb{y_i^s} = ({y_1^s,\cdots,y_L^s})$ is composed of L tokens.
The set of tokens (vocabulary) is denoted as $\Omega$, $\mathbf{T}$ denotes the set of possible texts. $h(
\pmb{x_i},\pmb{y_i^s}) \in \mathcal{R}^+$ is the score associated by a human annotator to the candidate text $\pmb{y}_
i^s$ when comparing it with the reference text $\pmb{x_i}$. We aim at building an evaluation metric $f$ such that $f(
\pmb{x_i} ,\pmb{y_i})\in \mathbb{R}^{+}$.

## Evaluation of the proposed metrics.

To assess the relevance of an evaluation metric $f$, correlation with human judgment is considered to be one of the most
important criteria. Debate on the relative merits of different correlations for the evaluation of automatic metrics is
ongoing but classical correlation measures are Pearson, Spearman or Kendall tests. Two meta-evaluation strategies are
commonly used: (1) text-level correlation or (2)
system-level correlation.

## Your Task:

Benchmark the correlation of existing metrics with human scores. You can start
from [here](https://github.com/PierreColombo/nlg_eval_via_simi_measures). You can work on whatever generation task you
want: translation [4], data2text generation [3] , story generation [2].

## Your reads:
[0] A Pseudo-Metric between Probability Distributions based on Depth-Trimmed Regions G Staerman, P Mozharovskyi, P
Colombo, S Clémençon, F d'Alché-Buc

[1] Pierre Colombo, Nathan Noiry, Ekhine Irurozki, Stephan Clemencon What are the best systems? New perspectives on NLP
Benchmarking NeurIPS 2022

[2] Cyril Chhun, Pierre Colombo, Fabian Suchanek, Chloe Clavel Of Human Criteria and Automatic Metrics: A Benchmark of
the Evaluation of Story Generation (oral) COLING 2022

[3] Pierre Colombo, Chloé Clavel and Pablo Piantanida. InfoLM: A New Metric to Evaluate Summarization & Data2Text
Generation. Student Outstanding Paper Award (oral) AAAI 2022

[4] Pierre Colombo, Guillaume Staerman, Chloé Clavel, Pablo Piantanida. Automatic Text Evaluation through the Lens of
Wasserstein Barycenters. (oral) EMNLP 2021

[5] Pierre Colombo, Maxime Peyrard, Nathan Noiry, Robert West, Pablo Piantanida. The Glass Ceiling of Automatic
Evaluation in Natural Language Generation

[6] Hamid Jalalzai, Pierre Colombo , Chloe Clavel, Eric Gaussier, Giovanna Varni, Emmanuel Vignon, and Anne Sabourin.
Heavy-tailed representations, text polarity classification & data augmentation. NeurIPS 2020

[7] Alexandre Garcia,Pierre Colombo, Slim Essid, Florence d’Alché-Buc, and Chloé Clavel. From the token to the review: A
hierarchical multimodal approach to opinion mining. EMNLP 2020

[8] Pierre Colombo, Wojciech Witon, Ashutosh Modi, James Kennedy, and Mubbasir Kapadia. Affect-driven dialog generation.
NAACL 2019


